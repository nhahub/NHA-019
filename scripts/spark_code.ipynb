{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b58966c-644d-4969-a994-4d7868509b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.9.11-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (4.9 kB)\n",
      "Downloading psycopg2_binary-2.9.11-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m420.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.9.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90379c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ New Spark session created\n",
      "Spark version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "try:\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    if spark:\n",
    "        spark.stop()\n",
    "        print(\"Stopped existing Spark session\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaToHDFS_And_PostgreSQL_SmartFarming\") \\\n",
    "    .config(\"spark.sql.streaming.kafka.useDeprecatedOffsetFetching\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✓ New Spark session created\")\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d3f328-44e9-4370-8da9-8e5a04f44bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All queries stopped\n",
      "Attempting to connect to Kafka at: broker:29092\n",
      "Successfully connected to Kafka!\n",
      "Streaming started:\n",
      "  PostgreSQL: smart_farming.sensor_data\n",
      "  HDFS: hdfs://namenode:9000/user/smart_farming_data\n",
      "Batch 0: 438 records to PostgreSQL\n",
      "Batch 1: 7 records to PostgreSQL\n",
      "Batch 2: 4 records to PostgreSQL\n",
      "Batch 3: 10 records to PostgreSQL\n",
      "Batch 4: 10 records to PostgreSQL\n",
      "Batch 5: 10 records to PostgreSQL\n",
      "Batch 6: 10 records to PostgreSQL\n",
      "Batch 7: 10 records to PostgreSQL\n",
      "Batch 8: 10 records to PostgreSQL\n",
      "Batch 9: 10 records to PostgreSQL\n",
      "Batch 10: 10 records to PostgreSQL\n",
      "Batch 11: 10 records to PostgreSQL\n",
      "Batch 12: 10 records to PostgreSQL\n",
      "Batch 13: 9 records to PostgreSQL\n",
      "Batch 14: 10 records to PostgreSQL\n",
      "Batch 15: 10 records to PostgreSQL\n",
      "Batch 16: 10 records to PostgreSQL\n",
      "Batch 17: 10 records to PostgreSQL\n",
      "Batch 18: 10 records to PostgreSQL\n",
      "Batch 19: 10 records to PostgreSQL\n",
      "Batch 20: 10 records to PostgreSQL\n",
      "Batch 21: 10 records to PostgreSQL\n",
      "Batch 22: 10 records to PostgreSQL\n",
      "Batch 23: 10 records to PostgreSQL\n",
      "Batch 24: 9 records to PostgreSQL\n",
      "Batch 25: 10 records to PostgreSQL\n",
      "Batch 26: 10 records to PostgreSQL\n",
      "Batch 27: 10 records to PostgreSQL\n",
      "Batch 28: 10 records to PostgreSQL\n",
      "Batch 29: 10 records to PostgreSQL\n",
      "Batch 30: 10 records to PostgreSQL\n",
      "Batch 31: 10 records to PostgreSQL\n",
      "Batch 32: 10 records to PostgreSQL\n",
      "Batch 33: 10 records to PostgreSQL\n",
      "Batch 34: 9 records to PostgreSQL\n",
      "Batch 35: 10 records to PostgreSQL\n",
      "Batch 36: 10 records to PostgreSQL\n",
      "Batch 37: 10 records to PostgreSQL\n",
      "Batch 38: 10 records to PostgreSQL\n",
      "Batch 39: 10 records to PostgreSQL\n",
      "Batch 40: 10 records to PostgreSQL\n",
      "Batch 41: 10 records to PostgreSQL\n",
      "Batch 42: 10 records to PostgreSQL\n",
      "Batch 43: 9 records to PostgreSQL\n",
      "Batch 44: 10 records to PostgreSQL\n",
      "Batch 45: 10 records to PostgreSQL\n",
      "Batch 46: 10 records to PostgreSQL\n",
      "Batch 47: 10 records to PostgreSQL\n",
      "Batch 48: 10 records to PostgreSQL\n",
      "Batch 49: 10 records to PostgreSQL\n",
      "Batch 50: 10 records to PostgreSQL\n",
      "Batch 51: 10 records to PostgreSQL\n",
      "Batch 52: 9 records to PostgreSQL\n",
      "Batch 53: 10 records to PostgreSQL\n",
      "Batch 54: 10 records to PostgreSQL\n",
      "Batch 55: 10 records to PostgreSQL\n",
      "Batch 56: 10 records to PostgreSQL\n",
      "Batch 57: 10 records to PostgreSQL\n",
      "Batch 58: 10 records to PostgreSQL\n",
      "Batch 59: 10 records to PostgreSQL\n",
      "Batch 60: 10 records to PostgreSQL\n",
      "Batch 61: 10 records to PostgreSQL\n",
      "Batch 62: 9 records to PostgreSQL\n",
      "Batch 63: 10 records to PostgreSQL\n",
      "Batch 64: 9 records to PostgreSQL\n",
      "Batch 65: 9 records to PostgreSQL\n",
      "Batch 66: 10 records to PostgreSQL\n",
      "Batch 67: 10 records to PostgreSQL\n",
      "Batch 68: 7 records to PostgreSQL\n",
      "Batch 69: 6 records to PostgreSQL\n",
      "Batch 70: 8 records to PostgreSQL\n",
      "Batch 71: 6 records to PostgreSQL\n",
      "Batch 72: 9 records to PostgreSQL\n",
      "Batch 73: 7 records to PostgreSQL\n",
      "Batch 74: 7 records to PostgreSQL\n",
      "Batch 75: 9 records to PostgreSQL\n",
      "Batch 76: 7 records to PostgreSQL\n",
      "Batch 77: 10 records to PostgreSQL\n",
      "Batch 78: 9 records to PostgreSQL\n",
      "Batch 79: 7 records to PostgreSQL\n",
      "Batch 80: 7 records to PostgreSQL\n",
      "Batch 81: 6 records to PostgreSQL\n",
      "Batch 82: 7 records to PostgreSQL\n",
      "Batch 83: 8 records to PostgreSQL\n",
      "Batch 84: 7 records to PostgreSQL\n",
      "Batch 85: 8 records to PostgreSQL\n",
      "Batch 86: 7 records to PostgreSQL\n",
      "Batch 87: 8 records to PostgreSQL\n",
      "Batch 88: 6 records to PostgreSQL\n",
      "Batch 89: 8 records to PostgreSQL\n",
      "Batch 90: 8 records to PostgreSQL\n",
      "Batch 91: 8 records to PostgreSQL\n",
      "Batch 92: 10 records to PostgreSQL\n",
      "Batch 93: 10 records to PostgreSQL\n",
      "Batch 94: 9 records to PostgreSQL\n",
      "Batch 95: 10 records to PostgreSQL\n",
      "Batch 96: 10 records to PostgreSQL\n",
      "Batch 97: 10 records to PostgreSQL\n",
      "Batch 98: 10 records to PostgreSQL\n",
      "Batch 99: 10 records to PostgreSQL\n",
      "Batch 100: 10 records to PostgreSQL\n",
      "Batch 101: 9 records to PostgreSQL\n",
      "Batch 102: 10 records to PostgreSQL\n",
      "Batch 103: 10 records to PostgreSQL\n",
      "Batch 104: 10 records to PostgreSQL\n",
      "Batch 105: 10 records to PostgreSQL\n",
      "Batch 106: 10 records to PostgreSQL\n",
      "Batch 107: 10 records to PostgreSQL\n",
      "Batch 108: 10 records to PostgreSQL\n",
      "Batch 109: 10 records to PostgreSQL\n",
      "Batch 110: 10 records to PostgreSQL\n",
      "Batch 111: 9 records to PostgreSQL\n",
      "Batch 112: 10 records to PostgreSQL\n",
      "Batch 113: 10 records to PostgreSQL\n",
      "Batch 114: 10 records to PostgreSQL\n",
      "Batch 115: 10 records to PostgreSQL\n",
      "Batch 116: 10 records to PostgreSQL\n",
      "Batch 117: 9 records to PostgreSQL\n",
      "Batch 118: 10 records to PostgreSQL\n",
      "Batch 119: 10 records to PostgreSQL\n",
      "Batch 120: 10 records to PostgreSQL\n",
      "Batch 121: 10 records to PostgreSQL\n",
      "Batch 122: 10 records to PostgreSQL\n",
      "Batch 123: 9 records to PostgreSQL\n",
      "Batch 124: 10 records to PostgreSQL\n",
      "Batch 125: 10 records to PostgreSQL\n",
      "Batch 126: 10 records to PostgreSQL\n",
      "Batch 127: 9 records to PostgreSQL\n",
      "Batch 128: 10 records to PostgreSQL\n",
      "Batch 129: 10 records to PostgreSQL\n",
      "Batch 130: 10 records to PostgreSQL\n",
      "Batch 131: 10 records to PostgreSQL\n",
      "Batch 132: 10 records to PostgreSQL\n",
      "Batch 133: 10 records to PostgreSQL\n",
      "Batch 134: 9 records to PostgreSQL\n",
      "Batch 135: 10 records to PostgreSQL\n",
      "Batch 136: 10 records to PostgreSQL\n",
      "Batch 137: 10 records to PostgreSQL\n",
      "Batch 138: 10 records to PostgreSQL\n",
      "Batch 139: 10 records to PostgreSQL\n",
      "Batch 140: 10 records to PostgreSQL\n",
      "Batch 141: 9 records to PostgreSQL\n",
      "Batch 142: 10 records to PostgreSQL\n",
      "Batch 143: 10 records to PostgreSQL\n",
      "Batch 144: 10 records to PostgreSQL\n",
      "Batch 145: 10 records to PostgreSQL\n",
      "Batch 146: 10 records to PostgreSQL\n",
      "Batch 147: 10 records to PostgreSQL\n",
      "Batch 148: 9 records to PostgreSQL\n",
      "Batch 149: 10 records to PostgreSQL\n",
      "Batch 150: 10 records to PostgreSQL\n",
      "Batch 151: 10 records to PostgreSQL\n",
      "Batch 152: 10 records to PostgreSQL\n",
      "Batch 153: 10 records to PostgreSQL\n",
      "Batch 154: 10 records to PostgreSQL\n",
      "Batch 155: 10 records to PostgreSQL\n",
      "Batch 156: 9 records to PostgreSQL\n",
      "Batch 157: 10 records to PostgreSQL\n",
      "Batch 158: 10 records to PostgreSQL\n",
      "Batch 159: 10 records to PostgreSQL\n",
      "Batch 160: 10 records to PostgreSQL\n",
      "Batch 161: 10 records to PostgreSQL\n",
      "Batch 162: 10 records to PostgreSQL\n",
      "Batch 163: 9 records to PostgreSQL\n",
      "Batch 164: 10 records to PostgreSQL\n",
      "Batch 165: 4 records to PostgreSQL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StringType, DoubleType\n",
    "import psycopg2\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Stop all active streaming queries first\n",
    "spark = SparkSession.getActiveSession()\n",
    "if spark:\n",
    "    for query in spark.streams.active:\n",
    "        print(f\"Stopping query: {query.name}\")\n",
    "        query.stop()\n",
    "    print(\"All queries stopped\")\n",
    "\n",
    "# Clear checkpoints\n",
    "checkpoint_paths = [\n",
    "    \"/tmp/checkpoints/kafka_to_hdfs_smartfarming\",\n",
    "    \"/tmp/checkpoints/postgres_checkpoint\",\n",
    "    \"/tmp/checkpoints/hdfs_checkpoint\"\n",
    "]\n",
    "\n",
    "for path in checkpoint_paths:\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"Cleared checkpoint: {path}\")\n",
    "\n",
    "# Schema\n",
    "sensor_schema = StructType() \\\n",
    "    .add(\"sensor_id\", StringType()) \\\n",
    "    .add(\"timestamp\", StringType()) \\\n",
    "    .add(\"soil_moisture\", DoubleType()) \\\n",
    "    .add(\"soil_pH\", DoubleType()) \\\n",
    "    .add(\"temperature\", DoubleType()) \\\n",
    "    .add(\"rainfall\", DoubleType()) \\\n",
    "    .add(\"humidity\", DoubleType()) \\\n",
    "    .add(\"sunlight_intensity\", DoubleType()) \\\n",
    "    .add(\"pesticide_usage_ml\", DoubleType()) \\\n",
    "    .add(\"farm_id\", StringType()) \\\n",
    "    .add(\"region\", StringType()) \\\n",
    "    .add(\"crop_type\", StringType())\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaToHDFS_And_PostgreSQL_SmartFarming\") \\\n",
    "    .config(\"spark.sql.streaming.kafka.useDeprecatedOffsetFetching\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "topic_name = \"smart_farming_data\"\n",
    "\n",
    "kafka_bootstrap = \"broker:29092\"\n",
    "\n",
    "print(f\"Attempting to connect to Kafka at: {kafka_bootstrap}\")\n",
    "\n",
    "# Read from Kafka\n",
    "df_raw = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap) \\\n",
    "    .option(\"subscribe\", topic_name) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .option(\"kafka.session.timeout.ms\", \"30000\") \\\n",
    "    .option(\"kafka.request.timeout.ms\", \"40000\") \\\n",
    "    .option(\"kafka.default.api.timeout.ms\", \"60000\") \\\n",
    "    .option(\"maxOffsetsPerTrigger\", \"1000\") \\\n",
    "    .load()\n",
    "\n",
    "print(\"Successfully connected to Kafka!\")\n",
    "\n",
    "# Parse JSON\n",
    "df_parsed = df_raw.selectExpr(\"CAST(value AS STRING) AS json_str\") \\\n",
    "    .withColumn(\"data\", from_json(col(\"json_str\"), sensor_schema)) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# PostgreSQL writer function\n",
    "def write_to_postgres(batch_df, epoch_id):\n",
    "    if batch_df.isEmpty():\n",
    "        print(f\"Batch {epoch_id} is empty\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=\"smart_farming\",\n",
    "            user=\"admin\", \n",
    "            password=\"password\", \n",
    "            host=\"postgres\"\n",
    "        )\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        rows_written = 0\n",
    "        for row in batch_df.collect():\n",
    "            cur.execute(\"\"\"\n",
    "                INSERT INTO public.sensor_data (\n",
    "                    sensor_id, timestamp, soil_moisture, soil_ph, \n",
    "                    temperature, rainfall, humidity, sunlight_intensity, \n",
    "                    pesticide_usage_ml, farm_id, region, crop_type\n",
    "                )\n",
    "                VALUES (%s::uuid, %s::timestamp, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                ON CONFLICT (sensor_id) DO NOTHING;\n",
    "            \"\"\", (\n",
    "                row.sensor_id, \n",
    "                row.timestamp, \n",
    "                row.soil_moisture, \n",
    "                row.soil_pH,\n",
    "                row.temperature, \n",
    "                row.rainfall, \n",
    "                row.humidity, \n",
    "                row.sunlight_intensity,\n",
    "                row.pesticide_usage_ml,\n",
    "                row.farm_id,\n",
    "                row.region,\n",
    "                row.crop_type\n",
    "            ))\n",
    "            rows_written += 1\n",
    "        \n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        print(f\"Batch {epoch_id}: {rows_written} records to PostgreSQL\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error batch {epoch_id}: {str(e)}\")\n",
    "\n",
    "# Start streaming to PostgreSQL\n",
    "postgres_query = df_parsed.writeStream \\\n",
    "    .foreachBatch(write_to_postgres) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoints/postgres_checkpoint\") \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .start()\n",
    "\n",
    "# Start streaming to HDFS\n",
    "hdfs_output_path = \"hdfs://namenode:9000/user/smart_farming_data\"\n",
    "\n",
    "hdfs_query = df_parsed.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", hdfs_output_path) \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoints/hdfs_checkpoint\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .start()\n",
    "\n",
    "print(\"Streaming started:\")\n",
    "print(f\"  PostgreSQL: smart_farming.sensor_data\")\n",
    "print(f\"  HDFS: {hdfs_output_path}\")\n",
    "\n",
    "# Wait for termination\n",
    "spark.streams.awaitAnyTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
